# code: language=ansible
---
sololab:
  vars:
    # https://9to5answer.com/can-i-bypass-host-key-checking-in-ansible
    # https://github.com/ansible/ansible/issues/49254
    ansible_host_key_checking: False
    ansible_ssh_host_key_checking: False

    ## Podman
    podman:
      target_user: podmgr
      vars_package:
        podman:
          state: present
          include_cockpit_podman: true
      vars_cgroups_delegation:
        all_users:
          state: present
          present_override: false
          resources: cpu cpuset io memory pids
        per_user:
          state: skipped
          resources: io
      vars_pam_limits:
        state: present
      vars_sub_ids:
        state: present
        present_override: false
      vars_kernel_modules:
        state: present
        list:
          - ip_tables
      vars_app_config:
        podman:
          containers_conf:
            state: present
            content: |
              [containers]
              # log_driver="journald"
              # log_size_max="10m"
              shm_size="128m"
              tz="Asia/Shanghai"
      vars_service:
        podman_socket:
          user_scope:
            skipped: false
            enabled: true
            state: started
          system_scope:
            skipped: true
            enabled: false
            state: stopped
        systemd_lingering:
          state: enable

    # playbook vars:
    # common settings
    target_host:
      name: "{{ host }}"
      uid: 1001
      connection_id: "cloud-init eth0"

    internal_basedomain: infra.sololab
    external_basedomain: infra.sololab

    ## freeipa
    FreeIPA:
      pod: 
        state: present # present / absent
        absent_force: true # true / false
        systemd_service: freeipa # name of the systemd service
        kube_file_host: localhost # tmpl mounted inside podman runner container
        kube_file_tmpl: /KubeWorkShop/FreeIPA/aio-freeipa.yaml.j2
        helm_chart_ref: /HelmWorkShop/helm-charts/charts/freeipa
        helm_value_file: /HelmWorkShop/helm-charts/charts/freeipa/values-sololab.yaml
        helm_release_name: freeipa
        container_name: freeipa-freeipa
        hostname: ipa.infra.sololab
        hostIP: 192.168.255.10
      app:
        domain_name: infra.sololab
        realm: infra.sololab
        ds_password: P@ssw0rd
        admin_password: P@ssw0rd
        admin_dn: uid=admin,cn=users,cn=accounts,dc=infra,dc=sololab
        sysaccount_dn: uid=system,cn=sysaccounts,cn=etc,dc=infra,dc=sololab
        sysaccount_uid: system
        sysaccount_password: P@ssw0rd
        dns_forwarder: 192.168.255.1
        tsig_key_name: keySololab
        tsig_algorithm: hmac-sha256
        tsig_secret: j/2DR2zkVAyDHL2XjE731sMt9s6cmRhXE6niScAgHA0= # gen by tsig-keygen keySololab
        dns_zonename: infra.sololab.
        dns_rev_zonename: 255.168.192.in-addr.arpa.

    ## StepCA
    StepCA:
      pod:
        state: present # absent / present
        absent_force: true # true / false
        systemd_service: stepca # name of the systemd service
        kube_file_host: localhost # tmpl mounted inside podman runner container
        kube_file_tmpl: /KubeWorkShop/StepCA/aio-stepca.yaml.j2
        image: docker.io/smallstep/step-ca:latest
        hostIP: 192.168.255.11
      app:
        DOCKER_STEPCA_INIT_NAME: sololab
        DOCKER_STEPCA_INIT_ACME: true
        DOCKER_STEPCA_INIT_DNS_NAMES: localhost,step-ca.infra.sololab
        DOCKER_STEPCA_INIT_SSH: true
        DOCKER_STEPCA_INIT_REMOTE_MANAGEMENT: true
        DOCKER_STEPCA_INIT_PROVISIONER_NAME: admin
        DOCKER_STEPCA_INIT_PASSWORD: P@ssw0rd
      dns:
        nsupdate: true # true / false
        configs:
          - zone: infra.sololab
            record: step-ca
            value: 192.168.255.11

    ## traefik
    Traefik:
      pod:
        state: present # present / absent
        absent_force: false # true / false
        systemd_service: traefik # name of the systemd service
        kube_file_host: localhost # tmpl mounted inside podman runner container
        kube_file_tmpl: /KubeWorkShop/Traefik/aio-traefik.yaml.j2
        helm_chart_ref: /HelmWorkShop/helm-charts/charts/traefik
        helm_value_file: /HelmWorkShop/helm-charts/charts/traefik/values-sololab.yaml
        helm_release_name: traefik
        image: docker.io/library/traefik:v2.10
        hostIP: 192.168.255.11
      app:
        customize_root_CA: true # true / false
        Root_CA_socket_address: step-ca.infra.sololab:9000
        ACME_enabled: true
        # for stepca: https://smallstep.com/blog/private-acme-server/#pointing-clients-at-the-right-acme-directory-url
        # https://{ca-host}/acme/{provisioner-name}/directory
        # check provisioner-name by step ca provisioner list
        # for freeipa:  https://freeipa.org/page/V4/ACME#how-to-use
        # https://ipa-ca.<domain-name>/acme/directory
        external_ACME_URL: https://acme-v02.api.letsencrypt.org/directory
        external_ACME_email: admin@INFRA.SOLOLAB
        internal_ACME_URL: https://step-ca.infra.sololab:9000/acme/acme/directory
        internal_ACME_email: admin@INFRA.SOLOLAB
        log_level: INFO # DEBUG, INFO, WARN, ERROR, FATAL, PANIC
      dns:
        nsupdate: true # true / false
        configs:
          - zone: infra.sololab
            record: traefik
            value: 192.168.255.11
          - zone: infra.sololab
            record: cockpit
            value: 192.168.255.11
      ingress:
        tmpl:
          - /KubeWorkShop/Traefik/conf/dynamic/traefik-dashboard.yaml.j2
          - /KubeWorkShop/Traefik/conf/dynamic/traefik-cockpit.yaml.j2
        dashboard_basedomain: "{{ internal_basedomain }}"
        dashboard_subdomain: traefik
        dashboard_acmeResolver: internal
        dashboard_basicAuth: admin:$apr1$/F5ai.wT$7nFJWh4F7ZA0qoY.JZ69l1
        cockpit_basedomain: "{{ internal_basedomain }}"
        cockpit_subdomain: cockpit
        cockpit_address: 192.168.255.10:9090

    ## gitlab
    GitLab:
      pod:
        state: absent # present / absent
        absent_force: false # true / false
        systemd_service: gitlab # name of the systemd service
        kube_file_host: localhost # tmpl mounted inside podman runner container
        kube_file_tmpl: /KubeWorkShop/GitLab/aio-gitlab.yaml.j2
        container_name: gitlab-gitlab
        # dont use gitlab-ce:16.1.x
        # https://gitlab.com/gitlab-org/omnibus-gitlab/-/issues/7823
        image: docker.io/gitlab/gitlab-ee:16.2.0-ee.0 # docker.io/gitlab/gitlab-ce:15.11.9-ce.0
      app:
        GITLAB_OMNIBUS_CONFIG: |
          external_url 'https://gitlab.infra.sololab'
          nginx['listen_https'] = false
          nginx['listen_port'] = 80
          gitlab_rails['gitlab_shell_ssh_port'] = 22
          gitlab_rails['omniauth_enabled'] = true
      dns:
        nsupdate: false # true / false
        configs:
          - zone: infra.sololab
            record: gitlab
            value: 192.168.255.11
      ingress:
        tmpl:
          - /KubeWorkShop/GitLab/traefik-gitlab.yaml.j2
        basedomain: "{{ external_basedomain }}"
        subdomain: gitlab
        acmeResolver: internal
        backend: http://gitlab/

        # config:
        #   # nsupdate
        #   key_name: "{{ FreeIPA.app.tsig_key_name }}"
        #   key_algorithm: "{{ FreeIPA.app.tsig_algorithm }}"
        #   key_secret: "{{ FreeIPA.app.tsig_secret }}"
        #   server: "{{ FreeIPA.pod.hostIP }}"
        #   zone: "{{ FreeIPA.app.dns_zonename }}"
        #   record: "{{ Traefik.ingress.dashboard_subdomain }}"
        #   value: "{{ Traefik.pod.hostIP }}"
        #   type: 'A'
        #   state: present

    ## Jenkins
    Jenkins:
      pod: 
        state: present # present / absent
        absent_force: false # true / false
        systemd_service: jenkins # name of the systemd service
        kube_file_host: localhost # tmpl mounted inside podman runner container
        helm_chart_ref: /HelmWorkShop/helm-charts/charts/jenkins-server
        helm_value_file: /HelmWorkShop/helm-charts/charts/jenkins-server/values-sololab.yaml
        helm_release_name: jenkins
      dns:
        nsupdate: true # true / false
        configs:
          - zone: infra.sololab
            record: jenkins
            value: 192.168.255.11
      ingress:
        tmpl:
          - /KubeWorkShop/jenkins/traefik-jenkins.yaml.j2
        basedomain: "{{ external_basedomain }}"
        subdomain: jenkins
        acmeResolver: internal
        backend: http://jenkins:8080
    ## Pritunl
    # Pritunl:
    #   pod:
    #     state: present # present / absent
    #     absent_force: true # true / false
    #     systemd_service: pritunl # name of the systemd service
    #     kube_file_host: localhost # tmpl mounted inside podman runner container
    #     kube_file_tmpl: /KubeWorkShop/Pritunl/aio-pritunl.yaml
    #     container_name: pritunl-pritunl
    #   dns:
    #     nsupdate: true # true / false
    #     configs:
    #       - zone: infra.sololab
    #         record: pritunl
    #         value: 192.168.255.10
    #   ingress:
    #     tmpl:
    #       - /KubeWorkShop/Pritunl/traefik-pritunl.yaml.j2
    #     basedomain: "{{ external_basedomain }}"
    #     subdomain: pritunl
    #     acmeResolver: internal
    #     backend: http://pritunl/

    ## Drone CI
    # Drone:
    #   pod: 
    #     state: absent # present / absent
    #     absent_force: true # true / false
    #     systemd_service: drone # name of the systemd service
    #     kube_file_host: localhost # tmpl mounted inside podman runner container
    #     helm_chart_ref: /HelmWorkShop/helm-charts/charts/drone-server
    #     helm_value_file: /HelmWorkShop/helm-charts/charts/drone-server/values-sololab.yaml
    #     helm_release_name: drone
    #   dns:
    #     nsupdate: true # true / false
    #     configs:
    #       - zone: infra.sololab
    #         record: drone
    #         value: 192.168.255.10
    #   ingress:
    #     tmpl:
    #       - /KubeWorkShop/Drone/traefik-drone.yaml.j2
    #     basedomain: "{{ external_basedomain }}"
    #     subdomain: drone
    #     acmeResolver: internal
    #     backend: http://drone/

    # ## Drone Runner
    # DroneRunner:
    #   pod: 
    #     state: absent # present / absent
    #     absent_force: true # true / false
    #     systemd_service: drone-runner # name of the systemd service
    #     kube_file_host: localhost # tmpl mounted inside podman runner container
    #     helm_chart_ref: /HelmWorkShop/helm-charts/charts/drone-runner
    #     helm_value_file: /HelmWorkShop/helm-charts/charts/drone-runner/values-sololab.yaml
    #     helm_release_name: drone-runner

  hosts:
    localhost:
      ansible_connection: local
    vyos:
      ansible_host: 192.168.255.1
      ansible_connection: ansible.netcommon.network_cli
      ansible_network_os: vyos.vyos.vyos
      ansible_user: vagrant
      ansible_password: vagrant
    kube-0:
      ansible_user: vagrant
      ansible_host: localhost
      ansible_python_interpreter: /usr/bin/python3
      # ansible_ssh_private_key_file: ~/.ssh/ssh.key
    kube-1:
      ansible_user: vagrant
      # https://riptutorial.com/ansible/example/5733/inventory-with-username-and-password
      # ansible_ssh_pass: vagrant
      ansible_host: 192.168.255.31
      ansible_python_interpreter: /usr/bin/python3
      ansible_ssh_private_key_file: /tmp/ssh.key
    kube-2-admin:
      ansible_user: vagrant
      # https://riptutorial.com/ansible/example/5733/inventory-with-username-and-password
      # ansible_ssh_pass: vagrant
      ansible_host: 192.168.255.11
      ansible_python_interpreter: /usr/bin/python3
      ansible_ssh_private_key_file: ~/.ssh/admin.key
    kube-2:
      ansible_user: podmgr
      # https://riptutorial.com/ansible/example/5733/inventory-with-username-and-password
      ansible_ssh_pass: podmgr
      ansible_host: 192.168.255.11
      ansible_python_interpreter: /usr/bin/python3
      ansible_ssh_private_key_file: ~/.ssh/ssh.key
    kube-3:
      ansible_user: vagrant
      # https://riptutorial.com/ansible/example/5733/inventory-with-username-and-password
      ansible_ssh_pass: vagrant
      ansible_host: 192.168.255.12
      # ansible_python_interpreter: /usr/bin/python3
      # ansible_ssh_private_key_file:  ~/.ssh/vagrant
